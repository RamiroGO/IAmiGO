const textos = [
	{
		"text": [
			"Las IAs generativas de texto, como GPT-4, estructuran un concepto a través de un proceso llamado **entendimiento del lenguaje natural**. Aquí está cómo se hace:",
			"1. **Tokenización**: El texto de entrada se divide en unidades más pequeñas llamadas tokens. Un token puede ser una palabra, un carácter o un subconjunto de una palabra.",
			"2. **Embedding**: Cada token se convierte en un vector numérico en un espacio de alta dimensión. Este vector captura el significado del token en el contexto del texto de entrada.",
			"3. **Modelado de secuencia**: Los vectores de tokens se alimentan a una red neuronal, que aprende a predecir el siguiente token en una secuencia basándose en los tokens anteriores. Esto permite a la IA entender la estructura gramatical y semántica del lenguaje.",
			"4. **Atención**: La IA asigna diferentes pesos a los tokens en la secuencia, lo que le permite centrarse en los tokens más relevantes para la tarea en cuestión.",
			"5. **Generación de texto**: La IA selecciona el siguiente token en la secuencia basándose en las probabilidades aprendidas durante el entrenamiento. Este proceso se repite hasta que se genera una secuencia completa de tokens.",
			"6. **Decodificación**: Los tokens generados se convierten de nuevo en texto legible.",
			"Es importante destacar que las IA's generativas de texto no 'entienden' los conceptos de la misma manera que los humanos. En lugar de eso, aprenden patrones en los datos de texto y utilizan estos patrones para generar texto que parece coherente y significativo para los humanos. Sin embargo, no tienen una comprensión semántica profunda de los conceptos que están generando."
		],
		"type": ["concept"]
	}
]

const enlaces = [
	"https://www.youtube.com/watch?v=D_C7ALTSh6Q&ab_channel=FECYTciencia", // Los laberintos del cerebro
]